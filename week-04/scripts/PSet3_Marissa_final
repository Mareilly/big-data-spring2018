import jsonpickle
import tweepy
import pandas as pd
import os
os.chdir('/Users/mareilly/Desktop/big-data-spring2018/week-04')
from twitter_keys import api_key, api_secret
    auth = tweepy.AppAuthHandler(api_key, api_secret)
# wait_on_rate_limit and wait_on_rate_limit_notify are options that tell our API object to automatically wait before passing additional queries if we come up against Twitter's wait limits (and to inform us when it's doing so).
    api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)
def auth(key, secret):
  auth = tweepy.AppAuthHandler(key, secret)
  api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)
  # Print error and exit if there is an authentication error
  if (not api):
      print ("Can't Authenticate")
      sys.exit(-1)
  else:
      return api
api = auth(api_key, api_secret)

def get_tweets(
    geo,
    out_file,
    search_term = '',
    tweet_per_query = 100,
    tweet_max = 150,
    since_id = None,
    max_id = -1,
    write = False
  ):
  tweet_count = 0
  all_tweets = pd.DataFrame()
  while tweet_count < tweet_max:
    try:
      if (max_id <= 0):
        if (not since_id):
          new_tweets = api.search(
            q = search_term,
            rpp = tweet_per_query,
            geocode = geo
          )
        else:
          new_tweets = api.search(
            q = search_term,
            rpp = tweet_per_query,
            geocode = geo,
            since_id = since_id
          )
      else:
        if (not since_id):
          new_tweets = api.search(
            q = search_term,
            rpp = tweet_per_query,
            geocode = geo,
            max_id = str(max_id - 1)
          )
        else:
          new_tweets = api.search(
            q = search_term,
            rpp = tweet_per_query,
            geocode = geo,
            max_id = str(max_id - 1),
            since_id = since_id
          )
      if (not new_tweets):
        print("No more tweets found")
        break
      for tweet in new_tweets:
        all_tweets = all_tweets.append(parse_tweet(tweet), ignore_index = True)
        if write == True:
            with open(out_file, 'w') as f:
                f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\n')
      max_id = new_tweets[-1].id
      tweet_count += len(new_tweets)
    except tweepy.TweepError as e:
      # Just exit if any error
      print("Error : " + str(e))
      break
  print (f"Downloaded {tweet_count} tweets.")
  return all_tweets

# Set a Lat Lon
latlng = '42.359416,-71.093993' # Eric's office (ish)
# Set a search distance
radius = '5mi'
# See tweepy API reference for format specifications
geocode_query = latlng + ',' + radius
# set output file location
file_name = 'data/tweets.json'
# set threshold number of Tweets. Note that it's possible
# to get more than one
t_max = 2000

tweets = get_tweets(
  geo = geocode_query,
  tweet_max = t_max,
  write = True,
  out_file = file_name
)

def parse_tweet(tweet):
  p = pd.Series()
  if tweet.coordinates != None:
    p['lat'] = tweet.coordinates['coordinates'][0]
    p['lon'] = tweet.coordinates['coordinates'][1]
  else:
    p['lat'] = None
    p['lon'] = None
  p['location'] = tweet.user.location
  p['id'] = tweet.id_str
  p['content'] = tweet.text
  p['user'] = tweet.user.screen_name
  p['user_id'] = tweet.user.id_str
  p['time'] = str(tweet.created_at)
  return p

tweets.to_json('/Users/mareilly/Desktop/big-data-spring2018/week-04/data/tweets.json')
df = pd.read_json('/Users/mareilly/Desktop/big-data-spring2018/week-04/data/tweets.json')
df.head()
tweets.dtypes
tweets['location'].unique()

#PROBLEM 2 - CLEAN Z DATA

boston_list = df[df['location'].str.contains("Boston", case=False)]['location']
df['location'].replace(boston_list, 'Boston, MA', inplace = True)

#check that it's been replaced
# boston_list.value_counts()
df['location'].value_counts()

usa_list = df[df['location'].str.contains("United States", case=False)]['location']
mass_list = df[df['location'].str.contains("Massachusetts", case=False)]['location']
cambridge_list = df[df['location'].str.contains("Cambridge", case=False)]['location']
europe_list = df[df['location'].str.contains("Europe", case=False)]['location']
uk_list = df[df['location'].str.contains("UK", case=False)]['location']
dorchester_list = df[df['location'].str.contains("Dorchester", case=False)]['location']
medford_list = df[df['location'].str.contains("Medford", case=False)]['location']
ny_list = df[df['location'].str.contains("New York", case=False)]['location']
somerville_list = df[df['location'].str.contains("Somerville", case=False)]['location']
canada_list = df[df['location'].str.contains("Canada", case=False)]['location']
ca_list = df[df['location'].str.contains("California", case=False)]['location']



df['location'].replace(usa_list, 'United States', inplace = True)
df['location'].replace(mass_list, 'Massachusetts', inplace = True)
df['location'].replace(cambridge_list, 'Cambridge, MA', inplace = True)
df['location'].replace(europe_list, 'Europe', inplace = True)
df['location'].replace(uk_list, 'UK', inplace = True)
df['location'].replace(dorchester_list, 'Dorchester, MA', inplace = True)
df['location'].replace(medford_list, 'Medford, MA', inplace = True)
df['location'].replace(ny_list, 'New York', inplace = True)
df['location'].replace(somerville_list, 'Somerville, MA', inplace = True)
df['location'].replace(canada_list, 'Canada', inplace = True)
df['location'].replace(ca_list, 'California', inplace = True)

df['location'].value_counts()

#Removes missing tweet locations
cleaner_tweets=df[df['location'] != ""]
cleaner_tweets['location'].value_counts()
cleaner_tweets.count
#Removes duplicates
tweets[tweets.duplicated(subset = 'content', keep = False)]

#Take only the cleaned locations -- only locations that have a count greater than 10
# Value counts of locations into a new data frame
location_count_frame= cleaner_tweets['location'].value_counts().to_frame()
location_count_frame.head()

#Take locations with counts over 10 and place into another variable
pop_place = location_count_frame[location_count_frame['location']>10]
pop_place
pop_place.shape

#Plot Pie Chart
import matplotlib.pyplot as plt
colors = ["#d84577","#4eacd7",
          "#cf4e33","#894ea8","#cf8c42","#d58cc9",
          "#737632","#9f4b75","#c36960"]

plt.pie(pop_place['location'], labels=pop_place.index.get_values(), shadow=False, colors=colors)
plt.title('Tweet Locations within 5 mile radius of Building 9')
plt.axis('equal')
plt.tight_layout()
plt.show()

#clean duplicates
net[net.duplicated(subset = 'content', keep = False)]
net.drop_duplicates(subset = 'content', keep = False, inplace = True)

#PROBLEM 3
#plt.scatter(df.lat, df.lon,)

tweets_geo = tweets[tweets['lon'].notnull() & tweets['lat'].notnull()]
len(tweets_geo)
len(tweets)

plt.scatter(tweets_geo['lon'], tweets_geo['lat'], s = 25)
plt.show()

# v weak results

#PROBLEM 4
# Set a Lat Lon
latlng = '42.359416,-71.093993' # Eric's office (ish)
# Set a search distance
radius = '5mi'
# See tweepy API reference for format specifications
geocode_query = latlng + ',' + radius
# set output file location
file_name = 'data/tweets_weather.json'
# set threshold number of Tweets. Note that it's possible
# to get more than one
t_max = 2000

weather_tweets = get_tweets(
  geo = geocode_query,
  tweet_max = t_max,
  search_term = 'weather',
  write = True,
  out_file = file_name
  )

weather_tweets.to_json('/Users/mareilly/Desktop/big-data-spring2018/week-04/data/tweets_weather.json')
weather = pd.read_json('/Users/mareilly/Desktop/big-data-spring2018/week-04/data/tweets_weather.json')

#PROBLEM 5
#CLEAN Z DATA

boston_list = df[df['location'].str.contains("Boston", case=False)]['location']
df['location'].replace(boston_list, 'Boston, MA', inplace = True)

usa_list = weather[weather['location'].str.contains("United States", case=False)]['location']
mass_list = weather[weather['location'].str.contains("Massachusetts", case=False)]['location']
cambridge_list = weather[weather['location'].str.contains("Cambridge", case=False)]['location']
europe_list = weather[weather['location'].str.contains("Europe", case=False)]['location']
uk_list = weather[weather['location'].str.contains("UK", case=False)]['location']
dorchester_list = weather[weather['location'].str.contains("Dorchester", case=False)]['location']
medford_list = weather[weather['location'].str.contains("Medford", case=False)]['location']
ny_list = weather[weather['location'].str.contains("New York", case=False)]['location']
somerville_list = weather[weather['location'].str.contains("Somerville", case=False)]['location']
canada_list = weather[weather['location'].str.contains("Canada", case=False)]['location']
ca_list = weather[weather['location'].str.contains("California", case=False)]['location']

weather['location'].replace(usa_list, 'United States', inplace = True)
weather['location'].replace(mass_list, 'Massachusetts', inplace = True)
weather['location'].replace(cambridge_list, 'Cambridge, MA', inplace = True)
weather['location'].replace(europe_list, 'Europe', inplace = True)
weather['location'].replace(uk_list, 'UK', inplace = True)
weather['location'].replace(dorchester_list, 'Dorchester, MA', inplace = True)
weather['location'].replace(medford_list, 'Medford, MA', inplace = True)
weather['location'].replace(ny_list, 'New York', inplace = True)
weather['location'].replace(somerville_list, 'Somerville, MA', inplace = True)
weather['location'].replace(canada_list, 'Canada', inplace = True)
weather['location'].replace(ca_list, 'California', inplace = True)

weather['location'].value_counts()

cleaner_weather_tweets=weather[weather['location'] != ""]
cleaner_weather_tweets['location'].value_counts()

weather_tweets[weather_tweets.duplicated(subset = 'content', keep = False)]

#Take only the cleaned locations -- only locations that have a count greater than 10
# Value counts of locations into a new data frame
weather_count = cleaner_weather_tweets['location'].value_counts().to_frame()
weather_count.head()
#Take locations with counts over 10 and place into another variable
weather_place = weather_count[weather_count['location']>10]
weather_place
weather_place.count

#BOOM IT WORKED!
#PROBLEM 6

weather_tweets_geo = weather_tweets[weather_tweets['lon'].notnull() & weather_tweets['lat'].notnull()]
len(weather_tweets_geo)
len(weather_tweets)

plt.scatter(weather_tweets_geo['lon'], weather_tweets_geo['lat'], s = 25)
plt.show()

#PROBLEM 7

tweet_loc=tweets[tweets['location'] != "]tweet_loc.to_csv('tweets.csv',sep=;;index=False)
tweet_loc.to_csv('twitter_data.csv', sep=',', encoding='utf-8')
#^Having trouble getting df to write to csv. Ill check in with teaching staff.

weather.to_csv('weathertweets_data.csv', sep=',', encoding='utf-8')
